# =========================== Trainer ==========================
trainer:
  accelerator: gpu                                        # mps for M1 gpu, cpu, cuda
  devices: 1                                              # number of GPUs to use (mps supports only 1 device)
  max_epochs: 100                                         # number of training epochs
  precision: 16
  check_val_every_n_epoch: 1
  gradient_clip_val: 0
  num_sanity_val_steps: 2

# =========================== DataModule ==========================
datamodule:
  batch_size: 4                                           # batch size
  shuffle: true                                           # initial dataset shuffle
  num_workers: 5                                          # number of data loader workers
  drop_last: true                                         # drop last in training 

# =========================== Segmentation Model ==========================
model:
  model_name: unet                                        # segmentation model name (e.g. unet)
  backbone: resnet18                                      # backbone for segmentation model (e.g. resnet18)
  num_classes: 1                                          # number of segmentation classes (LEAVE 1)
  in_channels: 3                                          # number of input image channles
  weights: imagenet                                       # pretrained weight

# ========================= Loss + Optimizer + LR Scheduler ===========
loss:
  criterion: jaccard                                      # loss during training (jaccard is the best)
  mode: binary                                            # binary, multilabel, multiclass

optimizer:
  algorithm: sgd                                          # which optimizer to use
  lr: 0.001                                               # learning rate
  momentum: 0                                             # momentum
  weight_decay: 0                                         # weight decay

scheduler:
  algorithm: cosine_annealing_warm                        # learning rate scheduler algorithm   
  t_0: 10                                                 # number of iterations for the first restart (e.g. 10 means afer 10 epochs)
  t_mult: 2                                               # a factor increases t_i after a restart
  eta_min: 0                                              # minimum learning rate

# =========================== Data Augmentations ==========================
transform:
  input_size: [512, 512]                                  # model input size (int, list, tuple)
  hflip_p: .5                                             # horizontal flip probability
  scale_limit: 0.5                                        # rescaling limit
  rotate_limit: 0                                         # rotation limit
  shift_limit: 0.1                                        # shift limit
  random_crop_p: 0.5                                      # random crop prob
  shift_scale_rotate_p: 1                                 # shift scale and rotate probability
  border_mode: 0                                          # filling borders with value
  gauss_noise_p: 0.5                                      # gaussian noise probability
  perspective_p: 0.5                                      # perspective transformation probability
  one_of_p: 0.9                                           # probabilty of selecting one of CLAHE, RandomBrighntessCotrast, and RandomGamma
  blur_limit: 3                                           # gaussian blur limit
  mean: [0.485, 0.456, 0.406]                             # to not apply normalization put [0, 0, 0]
  std: [0.229, 0.224, 0.225]                              # to not apply normalization put [1, 1, 1]

# =========================== Callbacks ==========================
callbacks:
  # filename: "epoch={epoch}-step={step}-val_loss={loss/val:.3f}-val_iou={IoU/all/val:.3f}"   # for cuda
  filename: "epoch={epoch}-step={step}-val_loss={loss/val:.3f}"    # for MPS
  monitor: loss/val                                       # metric to monitor
  mode: min                                               # mode to monitor metric
  save_top_k: 5                                           # number of best ckpt to save
  patience: 10                                            # early stopping patience

